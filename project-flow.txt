Building Pipeline

1. Create a Github repo and clone it in a Local and add experiments.
2. Add src folder along with all components(run them indivisually)
3. Add data,models, reports directories to .gitignore file
4. Now git add, commit and push

Setting up dvc Pipeline (without param)

5. Create dvc.yaml file and stages to it
6. dvc init then do "dvc repro" to test the pipeline automation.(Check dvc dag)
7. Now git add, commit and push

Setting up the DVC pipeline with params

8. add param.yaml file
9. Add the param setpup
10. Do "dvc repro" again to test the pipeline along with the param
11. Now git add, commit and push

Experiments with DVC

12. pip install dvclive
13. Add dvclive code block

i) import dvclive and yaml
from dvclive import live
import yaml
ii) Add the load_params function and initiate "params" var in main
iii) Add below code block to the main
with Live(save_dvc_exp=True) as live
    live.log_metric('accuracy', accuracy_score(y_test,y_test))
    live.log_metric('precision', precision_score(y_test,y_test))
    live.log_metric('recall', recall_score(y_test,y_test))

    live.load_params(params)

14. Do "dvc exp run", It will create a new dvc.yml(if already not there) and directory(Each run will be considered 
    as an experiments)
15. Do "dvc exp show" on terminal to see the experiments or use extension in VSCode(Install DVC extension)
16. Do "dvc exp remove <exp-name>" to remove exp (Optional) | "dvc exp apply <exp-name>" to reproduce previous exp
17. Change params, re-run code(Produce new experiments)
18. Now git add, commit. push 

Adding a remote S3 storage to DVC 

19. Login to AWS Console
20. Create an IAM user
21. Create S3 Bucket
22. pip install dvc[s3]
23. pip install awscli
24. aws configure
25. dvc remote add -d dvcstore s3://<bucketname>
26. dvc commit, push the exp outcome that we want to keep
27.  Finally git add, commit and push 

